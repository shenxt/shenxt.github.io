<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Comments for MetFlow]]></title>
    <url>%2F2018%2F10%2F13%2F20181013-metflow-comment%2F</url>
    <content type="text"><![CDATA[If you have any question about MetFlow, please leave a message for us.]]></content>
      <categories>
        <category>English</category>
        <category>Metabolomics</category>
        <category>MetFlow</category>
      </categories>
      <tags>
        <tag>Metabolomics</tag>
        <tag>English</tag>
        <tag>MetFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用CSI:FingerID 进行代谢物鉴定]]></title>
    <url>%2F2018%2F07%2F07%2F20180707-CSI-FingerID%2F</url>
    <content type="text"><![CDATA[简单介绍CSI:FingerID是德国Friedrich Schiller University的Sebastian Böckera教授开发的一款基于机器学习和代谢物finger ID的代谢物鉴定软件。发表在PNAS上。最近因为需要使用它进行代谢物鉴定，因此下面对其原理以及使用做一个简单介绍。 原理CSI:FingerID的基本原来是使用FigerID来表示代谢物，代谢物的Figer ID是一串0 1 0 1的数字，将代谢物转变为一系列碎片表示，然后按照顺序排列，如果某个代谢物含有该碎片，则表示为1，没有该碎片，则表示为0。因此，使用figer ID，可以将代谢物用数字表示。 训练阶段 作者从标准品MS2数据库中提取代谢物，然后将其二级谱图转变为fragmentation tree，fragmentation tree的有关知识可以参考该文章，http://www.pnas.org/content/pnas/112/41/12580.full.pdf。随后，从代谢物的结构式出发，得到代谢物的finger ID，然后对于每一个finger ID的molecular property，对建立fragmentation tree和其之间的SVM模型。从而得到一个使用fragmentation tree预测finger ID的机器学习预测模型。 预测阶段 拿到一个未知代谢物的二级谱图，首选将其转换为fragmentation tree，然后代入预测模型，预测出其理论的molecular property，然后得到其预测的finger ID。 打分阶段 将PubChem以及其他可以拿到的数据库中的所有代谢物的fingerID拿到，然后将未知代谢物的预测fingerID和其进行匹配打分（打分规则具体可以看论文），从而得到每个未知代谢物的candidates。 软件使用 下载 CSI:FingerID作者使用Java写了本地的软件版本，可以直接从其官网上下载。选择自己电脑对应的版本即可。 打开软件 下载到本地之后，无需安装，直接解压缩，然后双击.exe文件即可（我使用的windowns 64位系统）。他们也写有说明文档，不多，一共四十多页，但是真正讲解怎么使用的可能不到20页，还是非常清楚简洁的。可以下载下来仔细看看。 开始使用 打开软件之后，界面如下图所示。 因为我是需要使用批量处理，因此我就从如何批量处理讲解他的使用。首先，该软件接收的文件类型包括txt以及mgf，msp是非常普遍以及常用的，因此，推荐大家使用mgf格式文件。大家可以使用我的示例数据，是R文件，包含5个2级谱图。大家可以点击此处下载。下载之后，需要在R中打开，并将其转变为符合软件要求的格式。 12345678910111213141516171819202122232425262728293031323334353637##先加载数据load(&quot;temp.ms2&quot;)ms2.pos &lt;- temp.ms2##然后将该数据转为软件所需格式，并写出为mgf格式for(i in 1:length(ms2.pos))&#123; cat(i, &quot; &quot;) temp.ms2 &lt;- ms2.pos[[i]] fn.save &lt;- paste0(temp.ms2[[1]][1,1], &apos;.mgf&apos;) # info &lt;- temp.ms2[[1]] spec &lt;- temp.ms2[[2]] # sink(fn.save) cat(&quot;BEGIN IONS\n&quot;) cat(paste(&quot;PEPMASS=&quot;, info[2,1], sep = &quot;&quot;)) cat(&quot;\n&quot;) cat(&quot;MSLEVEL=1\n&quot;) cat(&quot;CHARGE=1+\n&quot;) cat(c(info[2,1], 1)) cat(&quot;\n&quot;) cat(&quot;END IONS\n&quot;) cat(&quot;\n&quot;) cat(&quot;BEGIN IONS\n&quot;) cat(paste(&quot;PEPMASS=&quot;, info[2,1], sep = &quot;&quot;)) cat(&quot;\n&quot;) cat(&quot;MSLEVEL=2\n&quot;) cat(&quot;CHARGE=1+\n&quot;) for(idx in 1:nrow(spec))&#123; cat(paste(spec[idx, ], collapse = &apos; &apos;), &apos;\n&apos;, sep = &apos;&apos;) &#125; cat(&quot;END IONS\n&quot;) sink()&#125; 每个谱图单独输出为一个文件。 导入谱图 点击Batch import按钮（在软件左上角），然后选择要导入的mgf文件。所有导入谱图的信息都在左侧列出。 开始计算 点击Compute all按钮（在软件正上方），然后设置参数，参数都是比较常见的，比如polarity，加和物形式等。然后点击CSI：fingerID，并点击submit。开始计算。 导出结果 点击Export Results按钮。可以将鉴定结果导出为CSV格式。]]></content>
      <categories>
        <category>Chinese</category>
        <category>组学基础</category>
      </categories>
      <tags>
        <tag>Chinese</tag>
        <tag>组学基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据的读取和输出]]></title>
    <url>%2F2018%2F06%2F03%2FR-study3%2F</url>
    <content type="text"><![CDATA[数据的读取和输出我们使用R语言来进行数据处理，那么就需要将本地的数据读取到R中，当在R中进行一定的处理之后，我们也需要将处理之后的数据输出。因此，本文就给大家介绍一下，R语言中最基本的数据读取和输出的方法。 如何设置工作路径工作路径（work directory）是指当前R的文件夹地址在哪？如果想知道自己现在的工作路径是什么，可以使用下列代码： 1getwd() 就会显示出来当前的工作路径在哪里，意思就是你直接输出的各种那文件，就会直接输出到这个文件夹里面。 那么比如我们现在有文件在另外一个文件夹里，在F盘的test里面，我想从这个里面读取我的数据，那么我该怎么做呢？那就需要将路径设置在该文件夹下，使用下列代码： 1setwd(&quot;F:/test&quot;) 注意一定要是使用反斜杠。 记住这两个函数，getwd和setwd，那么就可以将工作路径设置在我们想要的任何文件夹下面了。下面我们开始介绍最为常见的几种文件格式在R语言中的读取和输出。 csv文件csv文件是我们平时最为常见的文件了。全称为逗号分隔符文件。在R中，有现成的函数可以读取它。我们假设现在有一个数据，data.csv，在F盘的test文件夹中。我们现在要读取它，那么可以使用下列代码： 123#首选我们把路径设置到相应文件夹setwd(&quot;F:/test&quot;)data &lt;- read.csv(file = &quot;data.csv&quot;, stringsAsFactors = FALSE) 对的，就是使用read.csv函数，该函数就是用来读取csv文件的函数。他的参数有很多，我们来选几个最为常用的来说明一下，其他的可以使用?read.csv来参考官方文档 file：就是指要读取的文件的名称，比如上面的例子就是data.csv。注意后缀名一定要在。 header：是指文件是否有列名。如果设置为TRUE，则将第一行读取为该文件的列名。 stringsAsFactors：是否把文件的非数值型数据读取为factor类型。一般来说，我们都不需要，如果读取之后，需要将其转换为factor类型，那么后续可以自己手动转变。因此这里一定要将该参数设置为FALSE。 那么我们怎么输出csv文件呢？这就用到另外一个函数write.csv： 12#将data数据输出为csv文件write.csv(x = data, file = &quot;data1.csv&quot;, row.names = FALSE) 我们也来简单介绍一下write.csv函数的常见参数含义： x：就是你要输出的数据的名字，比如我们输出的数据是data，因此就设置为data。 file：是要输出数据的文件名。比如我们将data这个数据输出为data1.csv。 row.names：是指输出的csv文件的行名是否要自动加上。如果设置为TRUE，那么输出的数据会加上行名，设置为FALSE，则不会加上行名。 一般来说，上面的read.csv和write.csv函数就可以满足大多数的csv文件的读取和输出要求了，但是如果文件很大的话，那么这两个函数就会显的速度比较慢了，因此有另外一个由wickham大神写的包readr就非常的厉害了。下面同样使用readr包读取同样的数据，大家可以自行感受一下两者速度的差异： 1234567#首先安装readrinstall.packages(&quot;readr&quot;)library(&quot;readr&quot;)#读取数据data &lt;- read_csv(file = &quot;data.csv&quot;)#输出数据readr::write_csv(x = data, path = &quot;data1.csv&quot;) readr包中的read_csv和write_csv分别用来读取和输出数据。参数更加简洁。因此，强烈推荐大家使用这两个函数来进行数据的读取和输出。 xls和xlsx跟csv相比，excle表格文件就更加普遍了。一般来说，有xlx和xlsx两种文件。在这里，我们直接就推荐wicham的另外一个包，readxl包来进行这两种数据的读取和输出。 1234567#首先安装readxlinstall.packages(&quot;readxl&quot;)library(&quot;readr&quot;)#读取数据data &lt;- readxl::read_excel(path = &quot;data.xlsx&quot;)#输出数据write_excel_csv(x = data, path = &quot;data2.xlsx&quot;) 需要注意的是，write_excel_csv函数是包含在包readr中的。向大家强烈推荐这两个包用来处理csv，xlsx，xls文件的处理。 text文件另外一种常见文件就是txt文件了。可以使用R中自带的函数read.table。 介绍一下read.table的常用参数： file：读取数据的名称。 header：是否要把第一行变为列名，默认为FALSE。 sep：列之间的分隔符是什么？默认是空格，也就是sep=””，还有另外两种比较常见的，逗号，sep=”,”，其实就是csv文件了，还有sep=”\t”，也就是制表符。如果读取数据有问题的话， 可以试着改变sep来看看。 总结我们常见的几种数据的读取和输出就介绍到这了，其实就两个包和一个函数，readr，readxl和read.table。所有数据及代码已放到github上，请猛戳阅读原文获得链接。]]></content>
      <categories>
        <category>Chinese</category>
        <category>R语言</category>
        <category>组学基础</category>
      </categories>
      <tags>
        <tag>Chinese</tag>
        <tag>组学基础</tag>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R语言入门]]></title>
    <url>%2F2018%2F06%2F03%2FR-study2%2F</url>
    <content type="text"><![CDATA[这篇文章开始介绍R的基本知识，来让初步学习R语言的同学们能够快速入门。已经入门的同学请略过本文，否则你将浪费人生中的十分钟。。。1. 数据结构向量 （vector）：R语言中的战斗机向量类型是R语言的核心，很多运算都涉及到向量。我们先来一个最简单的向量。直接在控制台（Rstudio左下角的那个地方）中输入：12x &lt;- 3x x就是一个向量，一维的，3就是元素，只是只有一个，光杆司令。123456y &lt;- c(1, 2, 3)ylength(y)y[1]y[3]x[1] “&lt;-”是R语言中赋予的符号，在这个公式中，意味着我们把3这个值赋给x这个变量名，这样x就是3，但是3不一定是x。我们再创建一个长度不是1的向量，123456y &lt;- c(1, 2, 3)ylength(y)y[1]y[3]x[1] y就是一个长度为3的向量。length函数是用来查看向量长度的函数。对于y向量来说，他的元素就是1,2,3。向量中的元素是有顺序的，第一个从1开始计算，比如我想要查看向量y的第一个元素，那么就使用y[1]。y[1]代表的就是向量y的第一个元素，那么同样的y的第三个元素就是y[3]。矩阵（matrix）：R中的矩阵概念跟数学中完全一样。矩阵可以理解为二维的向量，也就是包括行数和列数。下面我们举个栗子： 12m &lt;- rbind(c(1,2), c(2,3))m rbind是一个函数，我们简单介绍一下。rbind直接从字面上理解是row combine，也就是行组合。什么意思呢？就是把几个向量按照行从上到下组合起来。用图可以很好地理解：（灵魂画师上线。。。） rbind函数就是把向量1和2然后按照行组合起来，成为一个新的矩阵。从这上面可以看到必须满足两个条件，向量1和2必须长度一致。同样还有一个函数，cbind，大家应该猜到了，就是column combine，也就是把几个向量按照列从左到右组合。比如12m2 &lt;- cbind(c(1,2,3,4), c(2,3,4,5))dim(m2) dim是查看矩阵的行数和列数。那么，怎么查看矩阵的某个元素呢？和向量一样，需要使用[]来看，但是要看单个元素，得使用行数和列数来进行确定，比如:1m2[1,2] 就是指m2的第1行和第2列的元素是什么东东。那么，能够直接从矩阵中提取子向量和子矩阵呢？当然阔以：123456m2[2,]m2[,1]m2[c(1,2),]m2[2,]#就是指m2的第2行的所有元素m2[,1]#就是指m2的第1列的所有元素m2[c(1,2),]#就是指把m2的第1和第2行的矩阵 注意：#也就是井号，是注释的意思，也就是#后面的代码是不会运行的，所以如果你想要对代码进行注释说明，那么就可以像上面一样，加一个#号，然后在后面写下注释的内容。数据框（data.frame）数据框和矩阵是非常类似的，但是完全不一样的，就是矩阵中的元素类型必须一致，而数据框中的元素类型则可以不同。比如我们有3个同学的语文，数学成绩。第一列是同学的名字（字符型，character），第二列是语文成绩（数字型，numeric），第三列是数学成绩（数字型，numeric）。这时候就得使用数据框了。1234d &lt;- data.frame(name = c(&quot;Shen&quot;, &quot;Li&quot;, &quot;Tu&quot;),Yuwen = c(90, 95, 100),Shuxue = c(85,80,90))d 使用函数data.frame建立数据框。name，Yuwen，Shuxue是建立好之后的数据框d的列名。前面介绍了R语言中的最为常见的几种数据结构，差不多可以满足我们后面的需求了。后面再遇到新的东西，我们再介绍。2. 函数和其他语言一样，函数是R语言中的战斗机。和数学概念一样，y = f(x)，也就是输入变量x，然后经过函数f的处理，得到结果y。函数也是一个对象，比如：123fun &lt;- function(a, b) &#123;a + b&#125; fun就是一个最为简单的函数。他要做的就是把a和b加起来，输出a和b之和。在这里面，a和b是形参（形式参数），不是具体的参数。只是用来把位置占起来。那么我们使用fun来做一个简单的计算：1fun(1,2) 这时候1和2就是实参（实际参数），也就是把1和2带进去，计算1和2的和。很多函数（也就是功能）都是别人已经写好的，比如求和sum，求根sqrt等等。 函数写好之后，很多都在不同的包（package）中，下面就介绍一下R语言中的包。3. 包（package）R包，类似C、Python中库的概念，指包含特定领域的函数、数据、文档等的集合。通过调用包，可以直接使用包中现成的数据、函数等，使开发方便快捷高效。 R的强大在于包含了各种各样的包，使用包非常有利于便捷开发。 一些功能在现有的包中并不存在，需要自己实现，实现后通过打包方便代码的复用。 每个包涵括一个领域相关的函数数据文档等，通过包可以有效地组织代码结构，有利于开发。 在R中，所有的函数都是封装好，放在包中的，包是R语言互相交流的最好方法。下面简单介绍一下包的安装，使用，等我们后面会介绍怎么创建，分享自己的R包。如何找到自己的需要的R包大家要相信很多方法都已经是存在的了，所以你能想到的功能，很多已经被别人做过了，已经有了现成的工具可以使用，也就是R包。因此，如果想实现一个功能，首先要想到的就是是不是已经存在了这样的工具，而不是自己去写。那么如何找到自己所需要的R包呢，这时候就得靠搜索引擎了（谷歌一下，你就知道）。当然，在我大天朝，谷歌早已经404了，如果你能够使用，或者能够翻墙接触到这个万恶的网站，那是最好的，如果不行，那就用必应（必应内心OS：终于想我到了啊。。。），最次，使用百度（百度：嗯？）。如何安装R包R包一般会公开在三个地方： CRAN：The Comprehensive R Archive Network，这是R core team的R包官方存放地方。安装放在这些的R的网址是https://mirrors.tongji.edu.cn/CRAN/，怎么安装这些包呢？使用下面代码即可： 1install.packages(&quot;package.name&quot;)#将package.name替换为自己的要安装的包的名字即可。 Bioconductor：这个是大部分的生信相关的包存放的地方，网址https://www.bioconductor.org/，安装使用代码如下： 123## try http:// if https:// URLs are not supportedsource(&quot;https://bioconductor.org/biocLite.R&quot;)biocLite(&quot;package.name&quot;)#将package.name替换为自己的要安装的包的名字即可。 Github：CRAN和bioconductor都需要经过一定的审核，才可以发布，因此另外一个最为常用的代码托管网站，github，就更为常用友好了，只要把自己写的代码和包放在github上，就可以安装R包了。安装github上的R包，需要使用到R语言大神Hadley Wickham写的包，devtools了。因此，首选需要安装这个包，代码如下： 123install.packages(&quot;devtools&quot;)library(devtools)install_github(&quot;user.name/package.name&quot;)#其中user.name是github作者的用户名。 如何获得R包及函数的帮助文档安装包之后，如要观察这个包里面所有的函数，以及这个R的说明，比如对于我们刚才安装的sxtTools这个包。我们就可以使用下面代码看这个包里面都含有哪些函数：1help(package = &quot;sxtTools&quot;) 如果想看某个具体的函数的信息，可以使用下面的代码：1?installBioc 每个函数都有详细的说明，可以通过看这些详细的官方文档来学习怎么使用，如果想更便捷，那就直接上12345#### 下期预告##### 下期我们开始学习怎么读取，输出数据，敬请期待PS：所有的代码，都放在了我的github上，大家猛戳阅读原文获得代码]]></content>
      <categories>
        <category>Chinese</category>
        <category>R语言</category>
        <category>组学基础</category>
      </categories>
      <tags>
        <tag>Chinese</tag>
        <tag>组学基础</tag>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rstudio的使用]]></title>
    <url>%2F2018%2F06%2F03%2FR-study1%2F</url>
    <content type="text"><![CDATA[为什么要做标准化基于质谱的代谢组学数据由于各种因素，比如质谱信号漂移，色谱柱污染，等等，会导致采集的数据有很多的系统误差存在，这些系统误差如果不去除掉，会严重影响数据质量，并进而影响从数据中挖掘有效信息的能力。比如下面图就分别显示了批次内（intra-batch）以及批次间（inter-batch）的系统误差，可以看到系统误差还是非常严重和明显的。因此对于质谱代谢组学数据来说，数据的标准化（data normalization）是必不可少的一步。 代谢组学数据标准化的方法代谢组学数据进行标准化的方法大致可以分为三种。 内标标准化：在样品中加入内标，然后对所有的峰都使用该内标进行校正。但是这种方法使用一个或者几个内标对所有的峰进行标准哈，并不可靠，因此用的不多。 基于样品本身：比如使用样品中所有峰的平均值、中位值或者总和对所有峰进行校正。另外还有比如PQN等等方法。 第三种在代谢组学数据中最为常见的标准化方法是基于QC（质量控制）样本的数据标准化。简单来说，就是将所要采集的所有样本取等量混合起来，组成QC样本，然后在采集数据的时候，每隔一定数量的样品，插入一针QC样本。因为QC样本都是一样的，因此可以用QC样本来模拟数据采集过程中信号的变化。得到数据之后，对每一个峰（peak），都将QC作为训练集，然后建立预测模型，预测信号变化，从而对样品中的信号进行校正。 基于QC和支持向量机的数据标准化R包：MetNormalizer我再读博期间的第一个项目就是建立一个基于QC样品的数据标准化方法，我们最后选择了一个非常有用并且常用的机器学习方法，SVR（支持向量机回归），最后的方法我们做成了一个R包，MetNormalizer。具体的内容可以参考我的文章，Normalization and Integration of Large-Scale Metabolomics Data Using Support Vector Regression。下面就用MetNormalizer自带的示例数据示范如何使用。 安装MetNormalizerMetNormalizer的源代码托管在github上，可以直接从github安装。在R控制台中输入下列代码，进行安装。 1234if(!require(devtools))&#123;install.packages(&quot;devtools&quot;)&#125;devtools::install_github(&quot;jaspershen/MetNormalizer&quot;) 准备数据以MetNormalizer自带数据为例。 12345678#首先加载MetNormalizer包library(MetNormalizer)#设置工作路径setwd(&quot;F:/test&quot;)#根据自己实际情况设置路径data(DemoData, package = &quot;MetNormalizer&quot;)#输出数据为csv格式write.csv(data, &quot;data.csv&quot;, row.names = FALSE)write.csv(sample.info, &quot;sample.info.csv&quot;, row.names = FALSE) 其中data为MS1 peak table，可以来源于任何的处理软件，如XCMS，MS-DIAL等。注意前三列必须为name(峰的名字)，m/z和RT。然后其他列为样本的intensity。 sample.info为样品信息，用来提供样品的信息。一共三列，第一列sample.name(样品名字)，然后依次是injection.order和class。class用来指明样品的种类，”Subject”说明该样品为生物样品，注意S要大写；QC是指样品为QC样品。 开始处理数据然后开始运行函数。 1234MetNormalizer(minfrac.qc = 0, minfrac.sample = 0, threads = 3, peakplot = TRUE) 处理结果 最后所有的处理结果都存放在svr normalization result文件夹中。]]></content>
      <categories>
        <category>Chinese</category>
        <category>R语言</category>
        <category>组学基础</category>
      </categories>
      <tags>
        <tag>Chinese</tag>
        <tag>组学基础</tag>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用R语言下载KEGG数据库]]></title>
    <url>%2F2018%2F06%2F03%2F20180827-R-KEGG%2F</url>
    <content type="text"><![CDATA[最近在群里发现大家经常交流如何下载各种数据库，确实，数据库对做各种组学来说，确实是非常重要的，但是很多数据库的下载做的并不是那么友好。KEGG是我们平时接触最多，以及最受大家欢迎的数据库之一，因此，这次我把一个非常好用的R包，KEGGREST下载KEGG数据库的用法进行了总结。 简单介绍KEGGREST包是一个bioconductor包，是由bioconductor的核心小组成员之一，Dan Tenenbaum写的，主要就是用来下载KEGG数据库。链接如下，https://bioconductor.org/packages/release/bioc/html/KEGGREST.html。 安装KEGGREST可以直接使用Bioconductor提供的下载方式进行下载，在R控制面板中输入下列代码： 123## try http:// if https:// URLs are not supportedsource(&quot;https://bioconductor.org/biocLite.R&quot;)biocLite(&quot;KEGGREST&quot;) 如果出错，不能安装，可以考虑把https换为http，输入下列代码： 123## try http:// if https:// URLs are not supportedsource(&quot;http://bioconductor.org/biocLite.R&quot;)biocLite(&quot;KEGGREST&quot;) 安装成功之后，就可以使用了。 概览成功安装之后，这个包有自己的说明文档，输入下列代码可以打开说明文档。 1browseVignettes(&quot;KEGGREST&quot;) 文档如下： 输入下列代码： 12library(KEGGREST)listDatabases() 可以看到该包可以下载的数据类型： 可以看到KEGG中几乎所有数据库都可以下载下来，我们以我们最为常用的pathway和compound数据为例，来说明如何使用KEGGREST。 下载pathway信息我们知道pathway信息有不同的物种，因此我们首先需要弄清楚有哪些物种，可以使用以下代码获得包含哪些物种： 12org &lt;- keggList(&quot;organism&quot;)head(org) org是一个matrix，其中第三列是各个物种的名字，第二列是各个物种的简称。 下面我们就来获取人类所有的pathway。 可以看到人类的简称是hsa。下面再获得人类的所有通路的代码简称： 12hsa.pathway &lt;- keggLink(&quot;pathway&quot;, &quot;hsa&quot;)hsa.pathway &lt;- unique(hsa.pathway) hsa.pahtway就是所有人类pathway的代码简称。 然后使用keggGet函数就可以将每个pathway的信息全部爬去下来，比如第一个pathway是：path:hsa00010” 12hsa.pathway[1]hsa00010 &lt;- keggGet(dbentries = hsa.pathway[1])[[1]] hsa00010是一个list格式的数据，它的内容分别是： 1names(hsa00010) 与KEGG网页版的数据是一一对应的。 如果想要得到人类所有KEGG pahtway的信息，则可以使用下列代码得到，以为该函数每次最多只接受10个pathway的下载请求（推测是为了防止KEGG的下载崩溃），因此更为便捷的办法是，循环获得所有的pathway信息： 123456hsa.pathway.database &lt;- vector(mode = &quot;list&quot;, length = length(hsa.pathway))for(i in 1:length(hsa.pathway))&#123; cat(i, &quot; &quot;) hsa.pathway.database[[i]] &lt;- keggGet(dbentries = hsa.pathway[i])&#125; 最后可以将得到的信息保存下来： 1save(hsa.pathway.database, file = &quot;hsa.pathway.database&quot;) 下载compound信息首先需要获得所有代谢物的ID 12compound.id &lt;- keggList(&quot;compound&quot;)compound.id &lt;- names(compound.id) 然后仍然通过keggGet函数获得所有代谢物的详细信息： 123456kegg.compound.database &lt;- vector(mode = &quot;list&quot;, length = length(compound.id))for(i in 1:length(compound.id))&#123; cat(i, &quot; &quot;) kegg.compound.database[[i]] &lt;- keggGet(dbentries = compound.id[i])&#125; 得到的kegg.compound.database也是一个list数据，如果想要将其转换为datafrmae格式，然后输出位csv或者xlsx格式，自己进行转换即可。 写在后面的话这个包用来下载KEGG数据库真的是非常方便，以前用过，后来忘掉了，最近又找出来，为了防止自己忘掉，所以写了一篇文章来记录一下，其他的功能后面会再研究记录。]]></content>
      <categories>
        <category>Chinese</category>
        <category>R语言</category>
        <category>组学基础</category>
      </categories>
      <tags>
        <tag>Chinese</tag>
        <tag>组学基础</tag>
        <tag>R语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代谢组数据标准化包MetNormalizer介绍]]></title>
    <url>%2F2018%2F05%2F16%2Fmetabolomics-data-normalization%2F</url>
    <content type="text"><![CDATA[为什么要做标准化基于质谱的代谢组学数据由于各种因素，比如质谱信号漂移，色谱柱污染，等等，会导致采集的数据有很多的系统误差存在，这些系统误差如果不去除掉，会严重影响数据质量，并进而影响从数据中挖掘有效信息的能力。比如下面图就分别显示了批次内（intra-batch）以及批次间（inter-batch）的系统误差，可以看到系统误差还是非常严重和明显的。因此对于质谱代谢组学数据来说，数据的标准化（data normalization）是必不可少的一步。 代谢组学数据标准化的方法代谢组学数据进行标准化的方法大致可以分为三种。 内标标准化：在样品中加入内标，然后对所有的峰都使用该内标进行校正。但是这种方法使用一个或者几个内标对所有的峰进行标准哈，并不可靠，因此用的不多。 基于样品本身：比如使用样品中所有峰的平均值、中位值或者总和对所有峰进行校正。另外还有比如PQN等等方法。 第三种在代谢组学数据中最为常见的标准化方法是基于QC（质量控制）样本的数据标准化。简单来说，就是将所要采集的所有样本取等量混合起来，组成QC样本，然后在采集数据的时候，每隔一定数量的样品，插入一针QC样本。因为QC样本都是一样的，因此可以用QC样本来模拟数据采集过程中信号的变化。得到数据之后，对每一个峰（peak），都将QC作为训练集，然后建立预测模型，预测信号变化，从而对样品中的信号进行校正。 基于QC和支持向量机的数据标准化R包：MetNormalizer我再读博期间的第一个项目就是建立一个基于QC样品的数据标准化方法，我们最后选择了一个非常有用并且常用的机器学习方法，SVR（支持向量机回归），最后的方法我们做成了一个R包，MetNormalizer。具体的内容可以参考我的文章，Normalization and Integration of Large-Scale Metabolomics Data Using Support Vector Regression。下面就用MetNormalizer自带的示例数据示范如何使用。 安装MetNormalizerMetNormalizer的源代码托管在github上，可以直接从github安装。在R控制台中输入下列代码，进行安装。 1234if(!require(devtools))&#123;install.packages(&quot;devtools&quot;)&#125;devtools::install_github(&quot;jaspershen/MetNormalizer&quot;) 准备数据以MetNormalizer自带数据为例。 12345678#首先加载MetNormalizer包library(MetNormalizer)#设置工作路径setwd(&quot;F:/test&quot;)#根据自己实际情况设置路径data(DemoData, package = &quot;MetNormalizer&quot;)#输出数据为csv格式write.csv(data, &quot;data.csv&quot;, row.names = FALSE)write.csv(sample.info, &quot;sample.info.csv&quot;, row.names = FALSE) 其中data为MS1 peak table，可以来源于任何的处理软件，如XCMS，MS-DIAL等。注意前三列必须为name(峰的名字)，m/z和RT。然后其他列为样本的intensity。 sample.info为样品信息，用来提供样品的信息。一共三列，第一列sample.name(样品名字)，然后依次是injection.order和class。class用来指明样品的种类，”Subject”说明该样品为生物样品，注意S要大写；QC是指样品为QC样品。 开始处理数据然后开始运行函数。 1234MetNormalizer(minfrac.qc = 0, minfrac.sample = 0, threads = 3, peakplot = TRUE) 处理结果 最后所有的处理结果都存放在svr normalization result文件夹中。]]></content>
      <categories>
        <category>Chinese</category>
        <category>Metabolomics</category>
      </categories>
      <tags>
        <tag>Chinese</tag>
        <tag>Metabolomics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取MassBank数据]]></title>
    <url>%2F2018%2F04%2F20%2F2018-04-13-hexo-domain-name%2F2018-04-13-massbank-database-get%2F</url>
    <content type="text"><![CDATA[利用R语言爬去MassBank数据库直接把代码记录如下： 12345678910111213141516171819202122library(rvest)url &lt;- &quot;http://www.massbank.jp/SVN/OpenData/record/Athens_Univ/&quot;met.id &lt;- read_html(url)met.id &lt;- html_text(met.id)met.id &lt;- strsplit(x = met.id, split = &quot;\n&quot;)[[1]]met.id &lt;- met.id[grep(&quot;.txt&quot;, met.id)]met.id &lt;- stringr::str_trim(met.id)athens_univ &lt;- vector(mode = &quot;list&quot;, length = length(met.id))for(i in 1:length(athens_univ))&#123; cat(i, &quot; &quot;) temp.url &lt;- paste(&quot;http://www.massbank.jp/SVN/OpenData/record/Athens_Univ/&quot;, met.id[i], &quot;/&quot;, sep = &quot;&quot;) met.ms2 &lt;- read_html(temp.url) met.ms2 &lt;- html_text(met.ms2) met.ms2 &lt;- strsplit(x = met.ms2, split = &quot;\r\n&quot;)[[1]] athens_univ[[i]] &lt;- met.ms2&#125;names(athens_univ) &lt;- gsub(pattern = &quot;.txt&quot;, replacement = &quot;&quot;, x = met.id)]]></content>
      <categories>
        <category>Chinese</category>
        <category>Metabolomics</category>
      </categories>
      <tags>
        <tag>Chinese</tag>
        <tag>Metabolomics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客绑定域名]]></title>
    <url>%2F2018%2F04%2F13%2F2018-04-13-hexo-domain-name%2F</url>
    <content type="text"><![CDATA[如何将Hexo博客绑定自己的域名？构建基于Hexo和Github之后的博客之后，默认地址为example@github.io，可以将该博客绑定为自己的域名。比如我的博客地址域名为shenxt.me。 购买域名我是在godaddy上购买的，一般博客的域名可以使用.me结尾的域名，这个在个人博客上用的非常多。 域名设置购买域名之后，需要对该域名进行设置。在godaddy官网上，点击自己已经购买的域名，如下图所示点击管理DNS。 然后点击添加，分别添加两个记录。第一个记录 类型为A，名称为@，值为你该博客的IP地址，查询博客地址的IP地址，可以使用该网站，直接输入博客地址即可，如shenxt.github.io。 类型为CNAME，名称为www，值为博客地址，如shenxt.guthub.io。 然后保存即可。 博客设置在博客本地文件夹source下，新建一个空白文件，CNAME，没有后缀名，然后使用文本打开，输入自己的域名，比如shenxt.me，注意前面不需要任何的前缀。然后关闭。打开终端，像发布新的博客一样进行操作。123hexo chexo ghexo d 测试一般需要稍等一会，然后在浏览器中输入自己的域名，即可打开自己的博客。]]></content>
      <categories>
        <category>Chinese</category>
        <category>R</category>
        <category>Hexo</category>
        <category>博客</category>
      </categories>
      <tags>
        <tag>Chinese</tag>
        <tag>Github</tag>
        <tag>博客</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[西雅图之行]]></title>
    <url>%2F2018%2F04%2F13%2F2018-06-28-seatlle%2F</url>
    <content type="text"><![CDATA[如何将Hexo博客绑定自己的域名？构建基于Hexo和Github之后的博客之后，默认地址为example@github.io，可以将该博客绑定为自己的域名。比如我的博客地址域名为shenxt.me。 购买域名我是在godaddy上购买的，一般博客的域名可以使用.me结尾的域名，这个在个人博客上用的非常多。 域名设置购买域名之后，需要对该域名进行设置。在godaddy官网上，点击自己已经购买的域名，如下图所示点击管理DNS。 然后点击添加，分别添加两个记录。第一个记录 类型为A，名称为@，值为你该博客的IP地址，查询博客地址的IP地址，可以使用该网站，直接输入博客地址即可，如shenxt.github.io。 类型为CNAME，名称为www，值为博客地址，如shenxt.guthub.io。 然后保存即可。 博客设置在博客本地文件夹source下，新建一个空白文件，CNAME，没有后缀名，然后使用文本打开，输入自己的域名，比如shenxt.me，注意前面不需要任何的前缀。然后关闭。打开终端，像发布新的博客一样进行操作。123hexo chexo ghexo d 测试一般需要稍等一会，然后在浏览器中输入自己的域名，即可打开自己的博客。]]></content>
      <categories>
        <category>Chinese</category>
        <category>博客</category>
        <category>随记</category>
      </categories>
      <tags>
        <tag>Chinese</tag>
        <tag>博客</tag>
        <tag>随记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R管道函数]]></title>
    <url>%2F2017%2F10%2F09%2F2017-10-11.pipline.function%2F</url>
    <content type="text"><![CDATA[R中的管道函数管道函数是R语言中为了减少过多使用中间变量而创建的，R语言本身没有没有自带管道函数，第三方包中提供了管道函数的使用，我使用的是stringr包。以前不怎么使用，现在开始试着使用管道函数，因为可以使代码更加简洁，并且节省内存空间。下面以一个小的例子来进行说明，这个例子是从别人那拷贝而来。 如何使用管道函数123library(&quot;rvest&quot;)library(&quot;stringr&quot;)url&lt;- &quot;http://www.zyzw.com/twzs010.htm&quot; 在不久前的一篇关于中国世界文遗产仪表盘的案例中，我在目标网站上抓取了52个中国世界自然文遗产的名称。按照传统的引入中间变量的写法，代码应该是这样的： 12345678web&lt;-read_html(url,encoding=&quot;GBK&quot;)web1&lt;-html_nodes(web,&quot;b&quot;)content1&lt;-html_text(web1,trim = FALSE)content2&lt;-gsub(&quot;(\\n\\t|，|\\d|、)&quot;,&quot;&quot;,content1)content3&lt;-grep(&quot;\\S&quot;,content2,value=T)content4&lt;-str_trim(content3,side=&quot;both&quot;)content5&lt;-content4[1:54]content6&lt;-content5[setdiff(1:54,c(35,39))];content6]]></content>
      <categories>
        <category>Chinese</category>
        <category>R</category>
      </categories>
      <tags>
        <tag>Chinese</tag>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[偶然翻到的文章]]></title>
    <url>%2F2017%2F10%2F09%2F2017-10-09.poetry%2F</url>
    <content type="text"><![CDATA[整理笔记的发现别人都说翻看以前的空间觉得自己以前写的东西宛若智障，这两天在翻看以前的笔记，也有这种感觉，因此，该删的删，直到看到了这篇文章，是在15年6月26号写的，现在想起来，虽然只有两年多的时间，但是物是人非，恍如隔世啊。这篇正好其实写的还不错，自己以前也是一个文艺青年，所以不舍得删掉，放在博客，充实一下内容。 谁让我遇到了你谁让我遇到了你，在春末夏初，当柳枝恢复了颜色，在花落蝉鸣，那姗姗来迟的雨季。生命一直如此平淡，我也从未期望奇迹，我过着我自己的生活，人生对我来说也许就是密密麻麻的正楷书写的履历，波澜不惊，平淡无奇。时间像流水般流逝，流逝而变化的是我周围的环境，呼吸以及空气，沉寂的是我自己，看似乐观的外表，其实内心的孤寂，没有人会在意我的在意，直到我遇到了你，在那个我也记不清的独特而不浪漫的某个时刻的某个过去。从此我的脑海里便有了挥散不去的，让我回想的你的笑容，你的身影，以及，你并不夸张而又足够吸引我的的气息。我对你的爱，不知怎么形容，也许你会觉得俗气：不变专一。此情难抑，如荷塘蜻蜓戏水，随波而散的青色涟漪，像我手中的铅笔，涂画在围城上的白色封皮。不知你是否还记得，我总是小心翼翼，而你总是不睬不理，当然我知道的，这不是你的心意，正像你说的，我没有飞蛾的勇气，也不像叶芝那样，完全的放开自己。你知道我的心里，容不下别人，此爱唯你。我知道随着岁月的流去，你会看到如今挂在我口上的，也的的确确刻在我心中的那句：不离不弃。直到有一天，当我老了，牙齿脱落，四肢无力，我也会坚守着对你的爱，用尽最后一丝力气，我甚至可以想象有一天，当我魂归天际，藏在我心中的，除了此生我从未改变的对父母家人的爱，便是独一无二的，在未来的昨天我遇到的你谁让我遇到了你，窗外的秋雨拍打着五颜六色的雨伞，也像一个声音的到来打断了我的思绪，或许这就是命中注定，或许这就是天意，我不知道我为什么爱你，就像我不知道我为什么不爱你。感觉就是这么奇妙的难以，难以用话语，难以用铅笔，勾勒出色彩斑斓的奇妙的爱的境地。谁让我遇到了你是缘分，抑或是机遇，缘分是什么？是你我虽然并未见到过的上帝，他也许偶然在做高数题，而他给出的答案就是随机，随机抽取，在未来的日子里我们需要互相珍惜，珍惜这段来之不易的，上帝赋予我们的非同一般的意义。]]></content>
      <categories>
        <category>Chinese</category>
        <category>Life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>Chinese</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[鄂尔多斯之旅]]></title>
    <url>%2F2017%2F10%2F04%2F2017-10-03.in.erdos%2F</url>
    <content type="text"><![CDATA[重返内蒙虽然在呼和浩特待了四年的时间，度过了自己的大学生活，但是对内蒙古和呼市的了解却非常的匮乏，也仅仅限于内蒙古大学周围的那片区域。毕业已经四年多了，博士都快毕业了，但是自从毕业之后还没有到过内蒙来。终于趁着这个十一，回到了内蒙，当刚下飞机的那一刻，看到汉蒙双文的招牌，思维一下子就回到了在内大读书的时候。 第一次到鄂尔多斯第一次看到传说中的鄂尔多斯，第一次看到准格尔旗，确实不愧是中国百强县，显得很繁华，比地处中原腹地的卫辉好多了。卫辉曾经的辉煌也只在四五百年前了。如今更像一个破败的小城，找不到任何发展的生机和希望。不知道未来委会人民能否找到适合自己的发展方向和产业。卫辉当然也有很多优势，比如交通，地理位置等等。 一些感悟大学期间就知道内蒙人民喜欢喝酒，也确实酒量很好。但是因为当时在上学，喝酒的机会不是很多，但是这次来到鄂尔多斯之后，很多场合都需要喝酒，才发现大家的热情喝酒量真的是非常的好，另外就是对自己也有一个新的认识，我也是可以喝酒的，原来我对白酒真的是低酒不沾，而且觉得白酒除了辣之外，真的是没有任何的特点，没有任何好喝的一点，但是自从上次喝了半瓶白酒之后，醉的一塌糊涂，到现在为止，已经将近一年半的时间了，没有再喝过白酒，不过这次，感觉也还好，自己也还挺能喝，喝了不少，没有醉，对自己的酒量有了新的认识，只是毕竟酒不是什么好东西，以后，除了在一些必须的场合，其他时候，还是不要喝酒了。和内蒙的朋友们在一起真的很高兴，大家的热情也感染了我，希望以后每年都有机会回到这来，和大家里聊天，喝酒。]]></content>
      <categories>
        <category>Chinese</category>
        <category>Life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>Chinese</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MetDNA instruction]]></title>
    <url>%2F2017%2F09%2F09%2F2017-09-09-metdna-blog%2F</url>
    <content type="text"><![CDATA[Instruction of MetDNA (version 0.99.11) Xiaotao Shen, Zhengjiang Zhu 问题报告 如果有任何问题出现，请点击此处给我留言。留言要包括下面三个部分： 处理文件地址； 运行的代码截图； 出现问题的报错信息截图。 常见错误总结 数据存放文件夹必须命名为POS或者NEG。 sample.info的两列列名必须是sample.name和group。 如果有正负离子模式数据，必须保证正负离子模式的样品名完全相同。 MetDNA处理步骤MetDNA一共处理过程一共分为了以下几个步骤： 数据检查； 二级谱图匹配：使用标准二级谱图数据库进行鉴定； 基于代谢反应网络的代谢物鉴定； 紊乱网络分析； 生成分析报告。 Ⅰ数据准备 MetDNA需要准备的数据包括一级数据peak table(csv格式)，二级数据(mgf格式或者msp)和样品信息sample.info(csv格式)。点击下载正离子demo数据和负离子demo数据。 注意： 如果正负离子模式数据都有，最后想使用正负离子模式的鉴定结果进行pathway分析，那么需要保证正负离子模式的样品名称保持一致，也就是同一个样品在正负离子模式数据中一样 Table 1: demo数据信息 组别 个数 含义 QC 8 QC W03 10 野生型3天 W30 10 野生型30天 E03 10 突变型E3天 E30 10 突变型E30天 P03 10 突变型P3天 P30 10 突变型P30天 1. 一级数据(data.csv) 一级数据可以是使用XCMS，MZmine，MS-DIAL或者其他软件处理之后的数据。第一列必须为代谢物峰的名字，”name”，第二列为”mz”，第三列为保留时间(RT)，且单位必须为秒，其他为样品的峰强度，除此之外不需要其他任何信息，且前三列命名必须为”name”，”mz”， “rt”。 2. 二级数据 二级质谱原始数据可以是使用样品采集的DDA，DIA或者targeted MS/MS数据。对于DDA数据来说，也可以是分段采集的二级数据。对于DDA和targeted MS/MS数据来说，需要将质谱原始二级数据使用ProteoWizard软件转为mgf格式，转换时参数设置参考下图。对于DIA数据来说，可以使用MS-DIAL处理之后，将其输出的msp格式文件直接拿来使用。 3. 样品信息(sample.info) 样品信息是样品的分组信息。第一列是样品名，命名为，”sample.name”，第二列是样品的分组信息，命名为，”group”。样品信息为csv文件，命名为sample.info.csv。 Ⅱ 数据整理 如果是正离子数据，请建立一个新的文件夹，命名为”POS”，如果是负离子数据，请建立一个新的文件夹命名为”NEG”，然后将一级数据(必须命名为data.csv)，二级数据(mgf或者msp格式)和样品信息(必须命名为sample.info.csv)放置于此文件夹下。并将该文件夹设置为路径。现在MetDNA部署在小服务器上，因此需要将数据放在小服务器中(注意是labdata文件夹)。 Ⅲ 数据处理1. 只有正离子或者负离子数据 如果只有正离子或者负离子数据，那么请使用下面代码进行处理。所有的步骤可以使用一个函数MetDNA全部完成。 设置工作路径并加载MetDNA。123#设置工作路径并加载MetDNAsetwd(&quot;/mnt/data/samba/labdata/workreport/Shen Xiaotao/demo/fly/POS&quot;)library(MetDNA) 运行MetDNA处理数据。1234567891011MetDNA(polarity = &quot;positive&quot;, column = &quot;hilic&quot;, ce = &quot;30&quot;, use.default.md = TRUE, group = c(&quot;W03&quot;, &quot;W30&quot;), uni.test = &quot;t&quot;, correct = TRUE, p.cutoff = 0.01, species = &quot;dme&quot;, dn.analysis = FALSE, pathway.enrichment = TRUE) 参数含义如下： polarity：数据采集极性，”positive”，”negative”或者”both”。 column：使用的柱子类型，”hilic”或者”rp”。 ce：二级采集的碰撞能量，支持”10”，”15”，”20”，”25”，”30”，”35”，”35,15” (35±15)，”40”， “45”，”50”，”55”，”60”，”65”，”70”。 use.default.md：进行保留时间预测模型建立时，是否使用默认的分子描述符，如果设置为FALSE，则会根据你的数据自动选择分子描述符。 group：要对哪些分组的样品进行分析，注意，计算fold change时，使用后面的样品除以前面的样品。 uni.test：单变量分析的方法，”t”，Student t test；”wilcox”，Wilcox test。 correct：是否需要对p值进行FDR校正。 p.cutoff：选择dysregulated peak时的p值cutoff。 species：所研究样品的物种来源，”dme”，果蝇；”hsa”，人类；”mmu”，小鼠；”rat”，大鼠，”bta”，牛；”gga”，Gallus gallus (鸡)；”dre”，Danio rerio (斑马鱼)；”cel”，Caenorharomyces elegans (线虫)；”sce”，Saccharomyces cerevisaiae (酵母)； “ath”，Arabidopsis thaliana (拟南芥)；”smm”，Schistosoma mansoni；”pfa”，Plasmodum falciparum 3D7；”tbr”，Trypanosoma brucei；”eco”， Escherichia coli K-12 MG1655(大肠杆菌)；”ppu”，Pseudomonas putida KT2440；”syf”，Synechococcus elongatus。 dn.analysis：是否进行紊乱网络分析，如果正负离子模式数据都有，那么需要将其设置为FALSE，如果只有一个模式的数据，那么设置为TRUE。 pathway.enrichment：是否使用差异代谢物对应的代谢物直接进行pathway enrichment analysis，现在默认的是如果polarity为”positive”或者”negative”，则不进行分析，如果polarity为”both”，则进行分析。 注意： 因为现在只能对两组数据进行比较，因此group参数只能写两个，如果有三组数据，如A，B和C组，那么需要分来两次处理，第一次先比较A和B，即将group设置为c(“A”, “B”)，然后运行MetDNA，然后将已经得到的结果中的”Dysregulated_network_analysis_result”，”Pathway_enrichment_analysis_result”和”Analysis_report”重新命名，否则下次运行这两个文件夹中的内容会被覆盖掉，然后将group设置为c(“A”, “C”)，然后再次运行MetDNA。 如果只有一个模式的数据，那么dn.analysis参数设置为TRUE，如果正负离子模式数据都有，那么需要将其设置为FALSE。 如果只有一个模式的数据，那么pathway.enrichment参数设置为TRUE，如果正负离子模式数据都有，那么需要将其设置为FALSE。 进行pathway enrichment分析时，可以自定义用于pathway分析的差异代谢物峰及其注释结果。在MetDNA中，默认是按照p值进行筛选差异代谢物峰，并使用差异代谢物峰的所有注释结果进行后续的通路分析，如果想自己按照其他方式筛选差异代谢物峰及其对应的注释，那么需要将自己筛选到的代谢物峰及其注释命名为”marker.csv”，然后将其放置在”POS”或者”NEG”文件夹中，重新运行MetDNA函数即可。 2. 正负离子模式数据都有情况下的处理 如果正负离子模式数据都有的话，那么需要首先将正负离子模式分别进行代谢物注释，而不进行后续的紊乱网络分析和通路分析，等正负离子模式数据都进行代谢物注释之后，再将其合并起来，进行后续分析。 首先分析正离子数据。1234567891011setwd(&quot;/mnt/data/samba/labdata/workreport/Shen Xiaotao/demo/fly/POS&quot;)library(MetDNA)MetDNA(polarity = &quot;positive&quot;, column = &quot;hilic&quot;, group = c(&quot;W03&quot;, &quot;W30&quot;), uni.test = &quot;t&quot;, correct = TRUE, p.cutoff = 0.01, species = &quot;dme&quot;, dn.analysis = FALSE, pathway.enrichment = FALSE) 然后分析负离子数据。12345678910setwd(&quot;/mnt/data/samba/labdata/workreport/Shen Xiaotao/demo/fly/NEG&quot;)MetDNA(polarity = &quot;negative&quot;, column = &quot;hilic&quot;, group = c(&quot;W03&quot;, &quot;W30&quot;), uni.test = &quot;t&quot;, correct = TRUE, p.cutoff = 0.01, species = &quot;dme&quot;, dn.analysis = FALSE, pathway.enrichment = FALSE) 然后合并分析。123456789MetDNA(polarity = &quot;both&quot;, column = &quot;hilic&quot;, group = c(&quot;W03&quot;, &quot;W30&quot;), uni.test = &quot;t&quot;, correct = TRUE, p.cutoff = 0.01, species = &quot;dme&quot;, dn.analysis = FALSE, pathway.enrichment = TRUE) 注意： 参数含义可以参考上文。 group的设置和注意事项也请参考上文。 Ⅳ 运行结果 MetDNA函数运行结束之后，所有的运行结果都存放在设置的路径中，包含二级谱图匹配鉴定结果，MRN注释结果，dysregulated network分析结果，pathway enrichment结果以及分析报告。如图5所示。点击此处观察分析结果结构图。 1 MetDNA.parameters.csv记录此次运行所使用的参数。2 MS2_match_result (二级谱图匹配结果) ms2.match.annotation.result.csv：二级谱图匹配之后的结果，与MetAnalyzer处理之后的结果相同； MS2_match_spectra：包含了所有二级谱图匹配结果图。 3 MRN_annotation_result (基于metabolic reacion network注释结果) MRN.annotation.result.csv：基于MRN的每一个peak的注释结果(Figure 6)。 Seed_Neighbor_MS2_match_spectra：包含的是每个Seed和他neighbor之间的二级谱图匹配结果。 MRN.annotation.result.csv其中的一些列的含义： Annotation.type：该peak的该注释的类型，其中seed代表是从二级谱图鉴定得到的，isotopeAnnotation是指是同位素峰注释得到的，adductAnnotation是指加合物峰注释得到的，metAnnotation是指邻近代谢物注释得到的； annotation.from.ID：该peak的该注释来自于哪个metabolite(ID); annotation.from.peak：该peak的该注释来自于哪个peak; ID：注释代谢物结果的KEGG ID; compound.name：注释结果的名字； isotope：同位素信息； adduct：加合物信息； Formula：化学结构式； score：注释打分； peak.group：peak group； confidence：对注释的peak group打分。 4 Dysregulated_network_analysis_result 1) volcano.plot是选取差异代谢物峰的火山图。每个代谢物峰的p值和fold change值可以从DNA.module.annotation.result.pos.csv中得到，或者在intermediate_data文件夹中有两个R文件，分别是p.value和fold.change。 2) DNA.module.annotation.result.pos.csv是通过dysregulated modules对注释结果进行筛选以及KEGG database注释之后的注释结果。其中每列的含义可以参考MRN.annotation.result.csv的说明。 3) DNA_module_information：module的一些结果，该文件夹结果暂时用不到，可以不看。 4) DNA_function_annotation：紊乱网络的定性分析和定量分析结果。具体内容如下： Cytoscape_data：dysregulated network用于cytoscape作图的文件。 Dysregulated_network_boxplot：每个pathway的定量信息box plot。 Dysregulated_network_heatmap：每个pathway的定量信息heatmap。 Dysregulated_network_heatmap：每个pathway的定量信息heatmap。 Quantitative_information：pathway的定量信息，以及每个代谢物对应的peak的信息。 DNA.pathway.enrichmetn.result.csv：dysregulated network通路富集结果。 5 Pathway_enrichment_analysis_result 1) volcano.plot是选取差异代谢物峰的火山图。 2) Pathway.enrichment.analysis.csv是使用差异代谢物峰对应的注释直接进行通路富集分析得到的富集结果。 3) Pathway.enrichment.MSEA.pdf是pathway富集分析的结果。 4) Pathway.enrichment.overview.pdf是pathway富集分析的结果的展示。 4) Pathway.heatmap.pdf是pathway定量结果的热图展示。 5) Boxplot文件夹：是每个通路的定量box plot展示。 6) Heatmap文件夹：是每个通路的定量heat map展示。 7) Quantitative_information：pathway的定量信息，以及每个代谢物对应的peak的信息。 6 Analysis_report对数据处理分析结果的总结。输出的结果存放在Analysis_report文件夹内。包括一份html格式的分析报告。]]></content>
      <categories>
        <category>R software</category>
        <category>Metabolomics</category>
      </categories>
      <tags>
        <tag>Chinese</tag>
        <tag>R</tag>
        <tag>Metabolomics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MetCleaning instruction]]></title>
    <url>%2F2017%2F09%2F09%2F2016-11-25-metcleaning%2F</url>
    <content type="text"><![CDATA[Introduction MetCleaning provides an integrated and automatic pipeline for data cleaning and statistical analysis of large scale mass spectrometry (MS) based-metabolomic data. It includes missing value (MV) filtering and imputation, zero value filtering, data normalization, data integration, data quality assessment, univariate statistical analysis, multivariate statistical analysis such as PCA and PLS-DA, potential marker selection and show. This document describes how to use the integrated functions, MetClean and MetStat in MetCleaning utilizing demo data. Installation and help MetCleaning is published in github (link). So you can install it via to github. code 1: Installation of MetCleaning 1234567891011121314##pcaMethods and impute should be installed form bioconductor##pcaMethossource(&quot;http://bioconductor.org/biocLite.R&quot;) biocLite(&quot;pcaMethods&quot;)##imputesource(&quot;http://bioconductor.org/biocLite.R&quot;) biocLite(&quot;impute&quot;) if(!require(devtools)) &#123; install.packages(&quot;devtools&quot;) &#125; library(devtools) install_github(&quot;jaspershen/MetCleaning&quot;) library(MetCleaning) help(package = &quot;MetCleaning&quot;) Data cleaning Data cleaning is integrated as a function named as MetClean in MetCleaning. We use the demo data as the example. Copy the code below and paste in you R console. code 2: Demo data of MetClean 123456789##demo datadata(data, package = &quot;MetCleaning&quot;)data(sample.information, package = &quot;MetCleaning&quot;)##demo work directorydir.create(&quot;Demo for MetCleaning&quot;)setwd(&quot;Demo for MetCleaning&quot;)##write fileswrite.csv(data, &quot;data.csv&quot;, row.names = FALSE)write.csv(sample.information , &quot;sample.information.csv&quot;, row.names = FALSE) The demo data have been added in your work directory and organized in you work directory as Figure 2 shows. It contains two files, “data.csv” and “sample.information.csv”. “data.csv” is the metabolomic dataset you want to process. Rows are features and columns are feature abundance of samples and information of features. The information of features must contain “name” (feature name), “mz” (mass to change ratio) and “rt” (retention time). Other information of features are optional, for example “isotopes” and “adducts”. The name of sample can contain “.”, but cannot contain “-“ and space. And the start of sample name cannot be number. For example, “A210.a” and “A210a” are valid, and “210a” or “210-a” are invalid. “sample.information.csv” is sample information for metabolomic dataset. Column 1 is “sample.name” which is the names of subject and QC samples. Please confirm that the sample names in “sample.information.csv” and “data.csv” are completely same. Column 2 is “injection.order” which is the injection order of QC and subject samples. Column 3 is “class”, which is used to distinguish “QC” and “Subject” samples. Column 4 is “batch” to provide acquisition batch information for samples. Column 5 is “group”, which is used to label the group of subject sample, for example, “control” and “case”. The “group” of QC samples is labeled as “QC”. Then you can run MetClean function to do data cleaning of data. All the arguments of MetClean can be found in the other functions in MetCleaning. You can use help(package = “MetCleaning”) to see the help page of MetCleaning. code 3: Running of MetClean 12##demo dataMetClean(polarity = &quot;positive&quot;) Running results of MetClean1.Missing or zero values filtering. In the missing or zero value filtering step, if there are samples which beyond the threshold you set, you should decide to filter them or not. We recommend to remove all of them as Figure 3 shows. 2.Sample filtering. In the QC or subject sample filtering step (based on PCA), if there are samples which beyond the threshold you set, you should decide to filter them or not. We don’t recommend to remove them as Figure 4 shows, because they should be consired combined other information. 3.Output files. Output files of MetClean are listed as Figure 5 shows.(1) “1MV overview”, “2MV filter”, “3Zero overview” and “4Zero filter” are missing and zero values filtering information.(2) “5QC outlier filter” and “6Subject outlier filter” are sample filtering based on PCA information.(3) “7Normalization result” is the data normalization information for each batch.(4) “8Batch effect” is the batch effect both in before and after data cleaning.(5) “9metabolite plot” is the scatter plot for each feature.(6) “10Data overview” is the overview of data.(7) “11RSD overview” is the RSD distribution for each batch both before and after data cleaning.(8) “data_after_pre.csv”, “qc.info.csv” and “subject.info” are the data and sample information after data cleaning.(9) “intermediate” is the intermediate data during processing. Statistical analysis Data statistical analysis is integrated as a function named as MetStat in MetCleaning. We use the demo data as the example. Please note that now MetStat can only process two class data. Copy the code below and paste in you R console. code 4: Demo data of MetStat 1234567data(&quot;met.data.after.pre&quot;, package = &quot;MetCleaning&quot;)data(new.group, package = &quot;MetCleaning&quot;)##create a folder for MetStat demodir.create(&quot;Demo for MetStat&quot;)setwd(&quot;Demo for MetStat&quot;)## export the demo data as csvwrite.csv(new.group, &quot;new.group.csv&quot;, row.names = FALSE) The demo data have been added in your work directory. “new.group.csv” is a sample.information which has been changed the group information you want to use for statistical analysis. For the sample which you don’t want to use them for statistical analysis, you can set they group information as NA like Figure 6 shows. code 5: Running of MetStat 1MetStat(MetFlowData = met.data.after.pre, new.group = TRUE) Running results of MetStat1.Sample removing. Firstly, you need to confirm the samples which you want to remove form dataset as Figure 7 shows. 2.Number of component selection in PLS-DA analysis. In PLS-DA analysis, you should manually select the best choice of the number of component. When the Console show “How many comps do you want to see?”, you can type 10 and enter “Enter” key. Then a MSE plot is showing, and the best number of component is the one has the smallest CV values. So type the number (in this example is 4) and enter “Enter” key. 3.Output files. Output files of MetStat are listed as Figure 9 shows.(1) “12PCA analysis” is the PCA score plot.(2) “13PLS analysis” contains the PLS-DA results.(3) “14heatmap” is the heatmap.(4) “15marker selection” contains the information of markers, volcano plot and boxplots of markers.(5) “data_after_stat.csv”, “qc.info.csv” and “subject.info” are the data and sample information after statistical analysis.(6) “intermediate” is the intermediate data during processing.]]></content>
  </entry>
  <entry>
    <title><![CDATA[我的第一篇博客]]></title>
    <url>%2F2017%2F09%2F09%2F2016-11-25-first-blog%2F</url>
    <content type="text"><![CDATA[我的博客我的博客使用markdown编写，使用的编辑器是ATOM，使用起来还是非常方便的。以后有时间，就用博客来记录我的学习，生活和工作。 申祖涛于上海 2016年11月25日晚8点十分]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用Github和Hexo建独立博客]]></title>
    <url>%2F2017%2F09%2F09%2F2017-09-09-github-blog%2F</url>
    <content type="text"><![CDATA[参考了一篇非常好的文章，然后结合自己的实际问题，讲解如何使用github结合hexo建立个人的独立博客。 1. Hexo介绍Hexo是基于NodeJs的静态博客框架，简单、轻量，其生成的静态网页可以托管在Github和Heroku上。 . 超快速度. 支持MarkDown. 一键部署. 丰富的插件 下面以我的博客为例，shenxt.github.io为例，讲解如何部署自己的博客。 2. 环境准备2.1 安装node.js去nodejs官网下载对应系统的安装包，按提示安装。 检验安装成功，在git shell中输入一下代码： 1$ node -v 2.2 安装hexo1$ npm install hexo-cli -g 如果是mac，则需要输入： 1$ sudo npm install hexo-cli -g 3. 利用Hexo搭建一个博客3.1 创建博客目录shenxt@github.io123$ hexo init shenxt.github.io$ cd limedroid.github.io$ npm install 3.2 生成静态页面12$ hexo clean$ hexo g # g is generate 3.3 运行1$ hexo s -p3600# is server 然后可以打开浏览器，输入地址 localhost:3600 即可看到效果。 4 发一篇文章试试4.1 穿件一个新的博客1$ hexo new test 此时会在source/posts目录下生成test.md文件，输入一些内容，然后保存。 然后看一下效果: 123$ hexo clean$ hexo g$ hexo s -p3600# is server 然后可以打开浏览器，输入地址 localhost:3600 即可看到效果。 5 配置网站的设置大部分都在_config.yml文件夹中，详细配置可以查看官方文档。 下面只列出简单常用配置: .title -&gt; 网站标题.subtitle -&gt; 网站副标题.description -&gt; 网站描述.author -&gt; 您的名字.language -&gt; 网站使用的语言 注意：进行配置时，需要在冒号:后加一个英文空格。 6 更换主题在网站配置文件_config.yml中，配置theme。 1theme: next next是主题的名字。Hexo有不同的人贡献主题，可以到其官方网站上下载不同主题。看中某一主题之后，直接点击其名字，进入到其github界面，然后复制其网址，使用下面代码，即可下载主题到本地。 1git clone https://github.com/fi3ework/hexo-theme-archer 然后将博客的配置文件theme修改为archer即可。 观察效果： 123$ hexo clean$ hexo g$ hexo s -p3600# is server 7 部署到github上7.1 在github网页版上创建和自己账户名相同的仓库，比如我的账户为shenxt，因此，创建的仓库为shenxt.github.io。7.2 安装hexo-deployer-git1$ npm install hexo-deployer-git --save 7.3 网站配置git在网上的配置文件_config.yml中配置deploy。 123type: gitrepo: https://github.com/shenxt/shenxt.github.iobranch: master 7.4 部署1$ hexo d# d is deploy 贴标签，方便搜索8.1 两个确认. 首先确认博客的配置文件中有： 1tag_dir: tags . 然后确认主题的配置文件有： 1tags: tags 8.2 新建tags页面1$ hexo new page tags 此时会在source/下生成tags/index.md文件。 8.3 修改source/tags/index.md1234title: tagsdate: 2015-10-20 06:49:50type: &quot;tags&quot;comments: false 8.4 在文章中添加tags在你的文章中添加： 1234tags: - Tag1 - Tag2 - Tag3 其文件头部类似于： 123456title: TagEditTextdate: 2016-11-19 10:44:25tags: - Tag1 - Tag2 - Tag3 9 分类，给文章归档9.1 两个确认. 确认博客配置文件打开了 1category_dir: categories . 确认主题配置文件打开了 1categories: /categories 9.2 新建categories文件1hexo new page categories 9.3 修改categories/index.md1234title: categoriesdate: 2015-10-20 06:49:50type: &quot;categories&quot;comments: false 9.4 在文章中添加categories在文章中添加： 12categories: - cate 其文件头部类似： 1234title: TagEditTextdate: 2016-11-19 10:44:25categories: - cate 10 添加评论功能这里推荐使用韩国的来必力系统。参考这个博客进行设置。]]></content>
      <categories>
        <category>github technology</category>
      </categories>
      <tags>
        <tag>Chinese</tag>
        <tag>Github</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Github Pages建独立博客]]></title>
    <url>%2F2017%2F09%2F09%2F2012-02-22-github-pages%2F</url>
    <content type="text"><![CDATA[Github很好的将代码和社区联系在了一起，于是发生了很多有趣的事情，世界也因为他美好了一点点。Github作为现在最流行的代码仓库，已经得到很多大公司和项目的青睐，比如jQuery、Twitter等。为使项目更方便的被人理解，介绍页面少不了，甚至会需要完整的文档站，Github替你想到了这一点，他提供了Github Pages的服务，不仅可以方便的为项目建立介绍站点，也可以用来建立个人博客。 Github Pages有以下几个优点： 轻量级的博客系统，没有麻烦的配置 使用标记语言，比如Markdown 无需自己搭建服务器 根据Github的限制，对应的每个站有300MB空间 可以绑定自己的域名 当然他也有缺点： 使用Jekyll模板系统，相当于静态页发布，适合博客，文档介绍等。 动态程序的部分相当局限，比如没有评论，不过还好我们有解决方案。 基于Git，很多东西需要动手，不像Wordpress有强大的后台 大致介绍到此，作为个人博客来说，简洁清爽的表达自己的工作、心得，就已达目标，所以Github Pages是我认为此需求最完美的解决方案了。 购买、绑定独立域名虽说Godaddy曾支持过SOPA，并且首页放着极其不专业的大胸美女，但是作为域名服务商他做的还不赖，选择它最重要的原因是他支持支付宝，没有信用卡有时真的很难过。 域名的购买不用多讲，注册、选域名、支付，有网购经验的都毫无压力，优惠码也遍地皆是。域名的配置需要提醒一下，因为伟大英明的GFW的存在，我们必须多做些事情。 流传Godaddy的域名解析服务器被墙掉，导致域名无法访问，后来这个事情在BeiYuu也发生了，不得已需要把域名解析服务迁移到国内比较稳定的服务商处，这个迁移对于域名来说没有什么风险，最终的控制权还是在Godaddy那里，你随时都可以改回去。 我们选择DNSPod的服务，他们的产品做得不错，易用、免费，收费版有更高端的功能，暂不需要。注册登录之后，按照DNSPod的说法，只需三步（我们插入一步）： 首先添加域名记录，可参考DNSPod的帮助文档：https://www.dnspod.cn/Support 在DNSPod自己的域名下添加一条A记录，地址就是Github Pages的服务IP地址：207.97.227.245 在域名注册商处修改DNS服务:去Godaddy修改Nameservers为这两个地址：f1g1ns1.dnspod.net、f1g1ns2.dnspod.net。如果你不明白在哪里修改，可以参考这里：Godaddy注册的域名如何使用DNSPod 等待域名解析生效 域名的配置部分完成，跪谢方校长。 配置和使用GithubGit是版本管理的未来，他的优点我不再赘述，相关资料很多。推荐这本Git中文教程。 要使用Git，需要安装它的客户端，推荐在Linux下使用Git，会比较方便。Windows版的下载地址在这里：http://code.google.com/p/msysgit/downloads/list。其他系统的安装也可以参考官方的安装教程。 下载安装客户端之后，各个系统的配置就类似了，我们使用windows作为例子，Linux和Mac与此类似。 在Windows下，打开Git Bash，其他系统下面则打开终端（Terminal）： 1、检查SSH keys的设置首先我们需要检查你电脑上现有的ssh key： $ cd ~/.ssh 如果显示“No such file or directory”，跳到第三步，否则继续。 2、备份和移除原来的ssh key设置：因为已经存在key文件，所以需要备份旧的数据并删除： $ ls config id_rsa id_rsa.pub known_hosts $ mkdir key_backup $ cp id_rsa* key_backup $ rm id_rsa* 3、生成新的SSH Key：输入下面的代码，就可以生成新的key文件，我们只需要默认设置就好，所以当需要输入文件名的时候，回车就好。 $ ssh-keygen -t rsa -C &quot;邮件地址@youremail.com&quot; Generating public/private rsa key pair. Enter file in which to save the key (/Users/your_user_directory/.ssh/id_rsa):&lt;回车就好&gt; 然后系统会要你输入加密串（Passphrase）： Enter passphrase (empty for no passphrase):&lt;输入加密串&gt; Enter same passphrase again:&lt;再次输入加密串&gt; 最后看到这样的界面，就成功设置ssh key了： 4、添加SSH Key到GitHub：在本机设置SSH Key之后，需要添加到GitHub上，以完成SSH链接的设置。 用文本编辑工具打开id_rsa.pub文件，如果看不到这个文件，你需要设置显示隐藏文件。准确的复制这个文件的内容，才能保证设置的成功。 在GitHub的主页上点击设置按钮： 选择SSH Keys项，把复制的内容粘贴进去，然后点击Add Key按钮即可： PS：如果需要配置多个GitHub账号，可以参看这个多个github帐号的SSH key切换，不过需要提醒一下的是，如果你只是通过这篇文章中所述配置了Host，那么你多个账号下面的提交用户会是一个人，所以需要通过命令git config --global --unset user.email删除用户账户设置，在每一个repo下面使用git config --local user.email &#39;你的github邮箱@mail.com&#39; 命令单独设置用户账户信息 5、测试一下可以输入下面的命令，看看设置是否成功，git@github.com的部分不要修改： $ ssh -T git@github.com 如果是下面的反应： The authenticity of host &apos;github.com (207.97.227.239)&apos; can&apos;t be established. RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48. Are you sure you want to continue connecting (yes/no)? 不要紧张，输入yes就好，然后会看到： Hi &lt;em&gt;username&lt;/em&gt;! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 6、设置你的账号信息现在你已经可以通过SSH链接到GitHub了，还有一些个人信息需要完善的。 Git会根据用户的名字和邮箱来记录提交。GitHub也是用这些信息来做权限的处理，输入下面的代码进行个人信息的设置，把名称和邮箱替换成你自己的，名字必须是你的真名，而不是GitHub的昵称。 $ git config --global user.name &quot;你的名字&quot; $ git config --global user.email &quot;your_email@youremail.com&quot; 设置GitHub的token2012-4-28补充：新版的接口已经不需要配置token了，所以下面这段可以跳过了 有些工具没有通过SSH来链接GitHub。如果要使用这类工具，你需要找到然后设置你的API Token。 在GitHub上，你可以点击Account Setting &gt; Account Admin： 然后在你的命令行中，输入下面的命令，把token添加进去： $ git config --global user.name &quot;你的名字&quot; $ git config --global user.token 0123456789your123456789token 如果你改了GitHub的密码，需要重新设置token。 成功了好了，你已经可以成功连接GitHub了。 使用GitHub Pages建立博客与GitHub建立好链接之后，就可以方便的使用它提供的Pages服务，GitHub Pages分两种，一种是你的GitHub用户名建立的username.github.io这样的用户&amp;组织页（站），另一种是依附项目的pages。 User &amp; Organization Pages想建立个人博客是用的第一种，形如beiyuu.github.io这样的可访问的站，每个用户名下面只能建立一个，创建之后点击Admin进入项目管理，可以看到是这样的：而普通的项目是这样的，即使你也是用的othername.github.io： 创建好username.github.io项目之后，提交一个index.html文件，然后push到GitHub的master分支（也就是普通意义上的主干）。第一次页面生效需要一些时间，大概10分钟左右。 生效之后，访问username.github.io就可以看到你上传的页面了，beiyuu.github.io就是一个例子。 关于第二种项目pages，简单提一下，他和用户pages使用的后台程序是同一套，只不过它的目的是项目的帮助文档等跟项目绑定的内容，所以需要在项目的gh-pages分支上去提交相应的文件，GitHub会自动帮你生成项目pages。具体的使用帮助可以参考Github Pages的官方文档： 绑定域名我们在第一部分就提到了在DNS部分的设置，再来看在GitHub的配置，要想让username.github.io能通过你自己的域名来访问，需要在项目的根目录下新建一个名为CNAME的文件，文件内容形如： beiyuu.com 你也可以绑定在二级域名上： blog.beiyuu.com 需要提醒的一点是，如果你使用形如beiyuu.com这样的一级域名的话，需要在DNS处设置A记录到207.97.227.245（这个地址会有变动，这里查看），而不是在DNS处设置为CNAME的形式，否则可能会对其他服务（比如email）造成影响。 设置成功后，根据DNS的情况，最长可能需要一天才能生效，耐心等待吧。 Jekyll模板系统GitHub Pages为了提供对HTML内容的支持，选择了Jekyll作为模板系统，Jekyll是一个强大的静态模板系统，作为个人博客使用，基本上可以满足要求，也能保持管理的方便，你可以查看Jekyll官方文档。 你可以直接fork我的项目，然后改名，就有了你自己的满足Jekyll要求的文档了，当然你也可以按照下面的介绍自己创建。 Jekyll基本结构Jekyll的核心其实就是一个文本的转换引擎，用你最喜欢的标记语言写文档，可以是Markdown、Textile或者HTML等等，再通过layout将文档拼装起来，根据你设置的URL规则来展现，这些都是通过严格的配置文件来定义，最终的产出就是web页面。 基本的Jekyll结构如下： |-- _config.yml |-- _includes |-- _layouts | |-- default.html | `-- post.html |-- _posts | |-- 2007-10-29-why-every-programmer-should-play-nethack.textile | `-- 2009-04-26-barcamp-boston-4-roundup.textile |-- _site `-- index.html 简单介绍一下他们的作用： _config.yml配置文件，用来定义你想要的效果，设置之后就不用关心了。 _includes可以用来存放一些小的可复用的模块，方便通过{ % include file.ext %}（去掉前两个{中或者{与%中的空格，下同）灵活的调用。这条命令会调用_includes/file.ext文件。 _layouts这是模板文件存放的位置。模板需要通过YAML front matter来定义，后面会讲到，{ { content }}标记用来将数据插入到这些模板中来。 _posts你的动态内容，一般来说就是你的博客正文存放的文件夹。他的命名有严格的规定，必须是2012-02-22-artical-title.MARKUP这样的形式，MARKUP是你所使用标记语言的文件后缀名，根据_config.yml中设定的链接规则，可以根据你的文件名灵活调整，文章的日期和标记语言后缀与文章的标题的独立的。 _site这个是Jekyll生成的最终的文档，不用去关心。最好把他放在你的.gitignore文件中忽略它。 其他文件夹你可以创建任何的文件夹，在根目录下面也可以创建任何文件，假设你创建了project文件夹，下面有一个github-pages.md的文件，那么你就可以通过yoursite.com/project/github-pages访问的到，如果你是使用一级域名的话。文件后缀可以是.html或者markdown或者textile。这里还有很多的例子：https://github.com/mojombo/jekyll/wiki/Sites Jekyll的配置Jekyll的配置写在_config.yml文件中，可配置项有很多，我们不去一一追究了，很多配置虽有用但是一般不需要去关心，官方配置文档有很详细的说明，确实需要了可以去这里查，我们主要说两个比较重要的东西，一个是Permalink，还有就是自定义项。 Permalink项用来定义你最终的文章链接是什么形式，他有下面几个变量： year 文件名中的年份 month 文件名中的月份 day 文件名中的日期 title 文件名中的文章标题 categories 文章的分类，如果文章没有分类，会忽略 i-month 文件名中的除去前缀0的月份 i-day 文件名中的除去前缀0的日期 看看最终的配置效果： permalink: pretty /2009/04/29/slap-chop/index.html permalink: /:month-:day-:year/:title.html /04-29-2009/slap-chop.html permalink: /blog/:year/:month/:day/:title /blog/2009/04/29/slap-chop/index.html 我使用的是： permalink: /:title /github-pages 自定义项的内容，例如我们定义了title:BeiYuu的博客这样一项，那么你就可以在文章中使用{ { site.title }}来引用这个变量了，非常方便定义些全局变量。 YAML Front Matter和模板变量对于使用YAML定义格式的文章，Jekyll会特别对待，他的格式要求比较严格，必须是这样的形式： --- layout: post title: Blogging Like a Hacker --- 前后的---不能省略，在这之间，你可以定一些你需要的变量，layout就是调用_layouts下面的某一个模板，他还有一些其他的变量可以使用： permalink 你可以对某一篇文章使用通用设置之外的永久链接 published 可以单独设置某一篇文章是否需要发布 category 设置文章的分类 tags 设置文章的tag 上面的title就是自定义的内容，你也可以设置其他的内容，在文章中可以通过{ { page.title }}这样的形式调用。 模板变量，我们之前也涉及了不少了，还有其他需要的变量，可以参考官方的文档：https://github.com/mojombo/jekyll/wiki/template-data 使用Disqus管理评论模板部分到此就算是配置完毕了，但是Jekyll只是个静态页面的发布系统，想做到关爽场倒是很容易，如果想要评论呢？也很简单。 现在专做评论模块的产品有很多，比如Disqus，还有国产的多说，Disqus对现在各种系统的支持都比较全面，到写博客为止，多说现在仅是WordPress的一个插件，所以我这里暂时也使用不了，多说与国内的社交网络紧密结合，还是有很多亮点的，值得期待一下。我先选择了Disqus。 注册账号什么的就不提了，Disqus支持很多的博客平台，参见下图： 我们选择最下面的Universal Code就好，然后会看到一个介绍页面，把下面这段代码复制到你的模板里面，可以只复制到显示文章的模板中： &lt;div id=&quot;disqus_thread&quot;&gt;&lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */ var disqus_shortname = &apos;example&apos;; // required: replace example with your forum shortname 这个地方需要改成你配置的网站名 /* * * DON&apos;T EDIT BELOW THIS LINE * * */ (function() { var dsq = document.createElement(&apos;script&apos;); dsq.type = &apos;text/javascript&apos;; dsq.async = true; dsq.src = &apos;http://&apos; + disqus_shortname + &apos;.disqus.com/embed.js&apos;; (document.getElementsByTagName(&apos;head&apos;)[0] || document.getElementsByTagName(&apos;body&apos;)[0]).appendChild(dsq); })(); &lt;/script&gt; &lt;noscript&gt;Please enable JavaScript to view the &lt;a href=&quot;http://disqus.com/?ref_noscript&quot;&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt; &lt;a href=&quot;http://disqus.com&quot; class=&quot;dsq-brlink&quot;&gt;blog comments powered by &lt;span class=&quot;logo-disqus&quot;&gt;Disqus&lt;/span&gt;&lt;/a&gt; 配置完之后，你也可以做一些异步加载的处理，提高性能，比如我就在最开始页面打开的时候不显示评论，当你想看评论的时候，点击“显示评论”再加载Disqus的模块。代码很简单，你可以参考我的写法。 $(&apos;#disqus_container .comment&apos;).on(&apos;click&apos;,function(){ $(this).html(&apos;加载中...&apos;); var disqus_shortname = &apos;beiyuu&apos;; var that = this; BYB.includeScript(&apos;http://&apos; + disqus_shortname + &apos;.disqus.com/embed.js&apos;,function(){$(that).remove()}); //这是一个加载js的函数 }); 如果你不喜欢Disqus的样式，你也可以根据他生成的HTML结构，自己改写样式覆盖它的，Disqus现在也提供每个页面的评论数接口，帮助文档在这里可以看到。 代码高亮插件如果写技术博客，代码高亮少不了，有两个可选插件DlHightLight代码高亮组件和Google Code Prettify。DLHightLight支持的语言相对较少一些，有js、css、xml和html，Google的高亮插件基本上任何语言都支持，也可以自定义语言，也支持自动识别，也有行号的特别支持。 Google的高亮插件使用也比较方便，只需要在&lt;pre&gt;的标签上加入prettyprint即可。所以我选择了Google Code Prettify。 搭建本地jekyll环境这里主要介绍一下在Mac OS X下面的安装过程，其他操作系统可以参考官方的jekyll安装。 作为生活在水深火热的墙内人民，有必要进行下面一步修改gem的源，方便我们更快的下载所需组建： sudo gem sources --remove http://rubygems.org/ sudo gem sources -a http://ruby.taobao.org/ 然后用Gem安装jekyll $ gem install jekyll 不过一般如果有出错提示，你可能需要这样安装： $ sudo gem install jekyll 我到了这一步的时候总是提示错误Failed to build gem native extension，很可能的一个原因是没有安装rvm，rvm的安装可以参考这里，或者敲入下面的命令： $ curl -L https://get.rvm.io | bash -s stable --ruby 然后还需要安装Markdown的解释器，这个需要在你的_config.yml里面设置markdown:rdiscount： $ gem install jekyll rdiscount 好了，如果一切顺利的话，本地环境就基本搭建完成了，进入之前我们建立的博客目录，运行下面的命令： $ jekyll serve --watch 这个时候，你就可以通过localhost:4000来访问了。还有关于jekyll bootstrap的资料，需要自己修改调试的，可以研究一下。 我在这个过程中还遇到两个诡异的没有解决的问题，一个是我放在根目录下面的blog.md等文件，在GitHub的pages服务上一切正常，可以通过beiyuu.com/blog访问的到，但是在本地环境下，总是not found，很是让人郁闷，看生成的_site目录下面的文件，也是正常的blog.html，但就是找不到，只有当我把URL改为localhost:4000/blog.html的时候，才能访问的到，环境不同真糟糕。 还有一个是关于category的问题，根据YAML的语法，我们在文章头部可以定义文章所属的类别，也可以定义为category:[blog,rss]这样子的多类别，我在本地试一切正常，但是push到GitHub之后，就无法读取了，真让人着急，没有办法，只能采用别的办法满足我的需求了。这里还有一篇Jekyll 本地调试之若干问题，安装中如果有其他问题，也可以对照参考一下。 结语如果你跟着这篇不那么详尽的教程，成功搭建了自己的博客，恭喜你！剩下的就是保持热情的去写自己的文章吧。]]></content>
  </entry>
  <entry>
    <title><![CDATA[使用R爬取HMDB和KEGG数据库]]></title>
    <url>%2F2017%2F09%2F09%2F2016-12-03-keggandhmdb%2F</url>
    <content type="text"><![CDATA[R语言爬虫 虽然相对于python来说，R语言爬虫并不是那么流行，但是对于比较小的数据爬取量，使用R还是很方便的。R的数据爬取比较流行的是利用XML和RCurl包进行爬取，在这篇博客里面，我就利用XML和RCurl包进行KEGG和HMDB的数据爬取。 爬取KEGG通路信息 因为我需要的信息是KEGG的通路信息，比较简单，也就是每个通路包含哪些代谢物，只要人的metaboloic pathway，因此，我需要先将KEGG中的通路的网页链接拿到。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748library(XML)library(RCurl)##从kegg主页上抓取代谢通路的urlURL = getURL(&quot;http://www.genome.jp/kegg/pathway.html#global&quot;)doc &lt;- htmlParse(URL,encoding=&quot;utf-8&quot;)xpath.a &lt;- &quot;//a/@href&quot;node &lt;- getNodeSet(doc, xpath.a)url1 &lt;- sapply(node, as.character)xpath.b &lt;- &quot;//a[@href]&quot;name &lt;- getNodeSet(doc, xpath.b)name &lt;- sapply(name, xmlValue)name2 &lt;- name[59:247]url2 &lt;- url1[59:247]url3 &lt;- url2[grep(&quot;show&quot;, url2)]pathwat.name &lt;- NULLmetabolite.id &lt;- list()metabolite.name &lt;- list()for (i in 1:length(url3)) &#123; cat(paste(i,&quot;/&quot;,length(url3))) cat(&quot;\n&quot;) URL &lt;- paste(&quot;http://www.genome.jp&quot;, url3[i], sep = &quot;&quot;) URL = getURL(URL) doc&lt;-htmlParse(URL,encoding=&quot;utf-8&quot;) xpath &lt;- &quot;//option[@value=&apos;hsa&apos;]&quot; node&lt;-getNodeSet(doc, xpath) if (length(node) ==0 ) &#123; cat(&quot;No human pathwat.&quot;) next() &#125;else&#123; URL &lt;- paste(&quot;http://www.genome.jp&quot;, url3[i], sep = &quot;&quot;) URL &lt;- gsub(pattern = &quot;map=map&quot;, replacement = &quot;map=hsa&quot;, x = URL) doc&lt;-htmlParse(URL,encoding=&quot;utf-8&quot;) xpath1 &lt;- &quot;//title&quot; node&lt;-getNodeSet(doc, xpath1) pathway.name[i] &lt;- xmlValue(node[[1]]) pathway.name[i] &lt;- substr(pathway.name[i], start = 2, stop = nchar(pathway.name[i])-1) xpath2 &lt;- &quot;//area[@shape=&apos;circle&apos;]/@title&quot; node&lt;-getNodeSet(doc, xpath2) metabolite &lt;- lapply(node, function(x) as.character(x)) metabolite.name[[i]] &lt;- substr(metabolite, start = 9, nchar(metabolite)-1) metabolite.id[[i]] &lt;- substr(metabolite, start = 1, stop = 6) &#125;&#125; 下面对爬取到的代谢通路进行筛选。 12345678idx &lt;- which(!is.na(pathway.name))pathway.name1 &lt;- pathway.name[idx]metabolite.id1 &lt;- metabolite.id[idx]metabolite.name1 &lt;- metabolite.name[idx]pathway.name2 &lt;- pathway.name1[-c(83,84)]metabolite.id2 &lt;- metabolite.id1[-c(83,84)]metabolite.name2 &lt;- metabolite.name1[-c(83,84)] 将爬取到的信息保存输出。 1234567891011121314151617181920212223242526met.name &lt;- NULLmet.id &lt;- NULLpath.name &lt;- NULLfor(i in 1:length(pathway.name2)) &#123; met.name[i] &lt;- paste(metabolite.name2[[i]], collapse = &quot;;&quot;) met.id[i] &lt;- paste(metabolite.id2[[i]], collapse = &quot;;&quot;) path.name[i] &lt;- gsub(pattern = &quot;KEGG PATHWAY: &quot;, &quot;&quot;, pathway.name2[i]) path.name[i] &lt;- substr(path.name[i], start = 1, stop = nchar(path.name[i])-23)&#125;kegg &lt;- data.frame(path.name, met.name, met.id)write.csv(kegg, &quot;kegg.csv&quot;, row.names = F)save(path.name, file = &quot;path.name&quot;)save(met.name, file = &quot;met.name&quot;)save(met.id, file = &quot;met.id&quot;)kegg.met &lt;- list()kegg.met[[2]] &lt;- sapply(path.name, list)kegg.met[[1]] &lt;- metabolite.name2kegg.met[[3]] &lt;- metabolite.id2names(kegg.met) &lt;- c(&quot;gs&quot;, &quot;pathwaynames&quot;, &quot;metid&quot;)save(kegg.met, file = &quot;kegg.met&quot;) 爬取HMDB通路信息 首先爬取HMDB的通路信息。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475##抓取HMDB通路信息library(XML)library(RCurl)hmdb.main &lt;- &quot;http://www.hmdb.ca/pathways?page=&quot;hmdb.main &lt;- paste(hmdb.main, c(2:46), sep = &quot;&quot;)hmdb.main &lt;- c(&quot;http://www.hmdb.ca/pathways&quot;, hmdb.main)##从HMDB主页上抓取代谢通路的urlpath.name &lt;- list()metabolite.id &lt;- list()spec &lt;- list()path.class &lt;- list()for (i in 40:length(hmdb.main)) &#123; cat(paste(&quot;page&quot;,i)) cat(&quot;:&quot;) URL = getURL(hmdb.main[i]) doc&lt;-htmlParse(URL,encoding=&quot;utf-8&quot;) xpath1 &lt;- &quot;//div[@class=&apos;panel-heading&apos;]&quot; node1 &lt;- getNodeSet(doc, xpath1) pathway.name &lt;- sapply(node1, xmlValue) cat(paste(length(pathway.name), &quot;pathways&quot;)) cat(&quot;\n&quot;) path.name[[i]] &lt;- pathway.name xpath2 &lt;- &quot;//div[@class=&apos;panel-body&apos;]&quot; node2 &lt;- getNodeSet(doc, xpath2) metabolite &lt;- sapply(node2, xmlValue) metabolite &lt;- unname(sapply(metabolite, function(x) &#123;gsub(&quot;Show&quot;, &quot; &quot;, x)&#125;)) idx &lt;- sapply(metabolite, function(x) &#123;gregexpr(&quot;HMDB[0-9]&#123;5&#125;&quot;, x)&#125;) met.id &lt;- list() for (j in 1:length(idx)) &#123; id &lt;- NULL for (k in 1:length(idx[[j]])) &#123; id[k] &lt;- substr(metabolite[j], idx[[j]][k], idx[[j]][k]+8) &#125; met.id[[j]] &lt;- id &#125; metabolite.id[[i]] &lt;- met.id xpath.a &lt;- &quot;//a[@class=&apos;link-out&apos;]/@href&quot; node&lt;-getNodeSet(doc, xpath.a) url1 &lt;- sapply(node, as.character) url1 &lt;- substr(url1, start = 1, stop = 29) url1 &lt;- url1[!duplicated(url1)] ###获取通路的人种和类别 species &lt;- NULL metabolic &lt;- NULL for (t in 1:length(url1)) &#123; cat(paste(&quot;t:&quot;,t));cat(&quot; &quot;) URL = getURL(url1[t]) doc &lt;- htmlParse(URL,encoding=&quot;utf-8&quot;) xpath &lt;- &quot;//div[@class=&apos;species&apos;]/text()&quot; node &lt;- getNodeSet(doc, xpath) species[t] &lt;- xmlValue(node[[1]]) xpath &lt;- &quot;//div[@id=&apos;des_subject&apos;]/text()&quot; node &lt;- getNodeSet(doc, xpath) metabolic[t] &lt;- xmlValue(node[[1]]) &#125; spec[[i]] &lt;- species path.class[[i]] &lt;- metabolic&#125; 对爬取到的代谢通路进行筛选。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051save(path.name, file = &quot;path.name&quot;)save(metabolite.id, file = &quot;metabolite.id&quot;)save(spec, file = &quot;spec&quot;)save(path.class, file = &quot;path.class&quot;)pathway.name &lt;- NULLmetabolite.ID &lt;- list()species &lt;- NULLpathway.class &lt;- NULLfor (i in 1:length(path.name)) &#123; pathway.name &lt;- c(pathway.name, path.name[[i]]) metabolite.ID &lt;- c(metabolite.ID, metabolite.id[[i]]) species &lt;- c(species, spec[[i]]) pathway.class &lt;- c(pathway.class, path.class[[i]])&#125;pathway.class &lt;- substr(x = pathway.class, 1, regexpr(&quot;\\\n&quot;, pathway.class)-1)metabolite.name &lt;- list()for (i in 1:length(metabolite.ID)) &#123; id &lt;- metabolite.ID[[i]] idx &lt;- match(id, hmdbdatabase[,1]) name &lt;- hmdbdatabase[idx,2] metabolite.name[[i]] &lt;- name&#125;a &lt;- unlist(lapply(metabolite.name, function(x) &#123;paste(x, collapse = &quot;;&quot;)&#125;))b &lt;- unlist(lapply(metabolite.ID, function(x) &#123;paste(x, collapse = &quot;;&quot;)&#125;))idx &lt;- grep(&quot;Metabolic&quot;, pathway.class)metabolite.name &lt;- metabolite.name[idx]metabolite.ID &lt;- metabolite.ID[idx]pathway.name &lt;- pathway.name[idx]pathway.class &lt;- pathway.class[idx]species &lt;- species[idx]hmdb.pathway &lt;- data.frame(pathway.name, pathway.class,a, b)[idx,]write.csv(hmdb.pathway, &quot;hmdb.pathway.csv&quot;)a &lt;- list()for (i in 1:length(pathway.name)) &#123; a[[i]] &lt;- pathway.name[i]&#125;pathway.name &lt;- ahmdb.met &lt;- list(gs = metabolite.name, pathwaynames = pathway.name, id = metabolite.ID)save(hmdb.met, file = &quot;hmdb.met&quot;) 爬取HMDB代谢物信息 首先，获得所有代谢物的页面链接。 12345678910111213141516171819202122232425###抓取HMDB代谢物信息library(XML)library(RCurl)hmdb.main &lt;- &quot;http://www.hmdb.ca/metabolites?c=hmdb_id&amp;d=up&amp;page=&quot;hmdb.main &lt;- paste(hmdb.main, c(2:1681), sep = &quot;&quot;)hmdb.main &lt;- c(&quot;http://www.hmdb.ca/metabolites&quot;, hmdb.main)##从HMDB主页上抓取代谢物的urlurl &lt;- NULLfor (i in 1:length(hmdb.main)) &#123; cat(i) cat(&quot; &quot;) URL = getURL(hmdb.main[i]) doc&lt;-htmlParse(URL,encoding=&quot;utf-8&quot;) xpath &lt;- &quot;//a[@href]/@href&quot; node&lt;-getNodeSet(doc, xpath) url1 &lt;- sapply(node, as.character) url1 &lt;- url1[grep(&quot;metabolites/HMDB&quot;, url1)] url1 &lt;- unique(url1) url &lt;- c(url, url1)&#125;url1 &lt;- paste(&quot;http://www.hmdb.ca/&quot;,url, sep = &quot;&quot;)save(url1, file = &quot;url1&quot;) 下面开始进行代谢物信息爬取。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970library(mailR)for (i in 1:400) &#123; cat(paste((i-1)*100+1,&quot;-&quot;,i*100,&quot;/&quot;, length(url1), sep = &quot;&quot;)) cat(&quot;\n&quot;) URL &lt;- getURL(url1[((i-1)*100+1):(i*100)]) doc &lt;- htmlParse(URL, encoding=&quot;utf-8&quot;) xpath1 &lt;- &quot;//tr&quot; node1 &lt;- getNodeSet(doc, xpath1) node1 &lt;- sapply(node1, xmlValue) HMDB_ID[((i-1)*100+1):(i*100)] &lt;- gsub(pattern = &quot;HMDB ID&quot;, replacement = &quot;&quot;,node1[grep(&quot;HMDB ID&quot;, node1)]) Common_Name[((i-1)*100+1):(i*100)] &lt;- gsub(&quot;Common Name&quot;, &quot;&quot;,node1[grep(&quot;Common Name&quot;, node1)]) temp &lt;- gsub(&quot;SynonymsValueSource&quot;, &quot;&quot;,node1[grep(&quot;Synonyms&quot;, node1)]) temp &lt;- gsub(&quot;Generator&quot;, &quot;;&quot;,temp) temp &lt;- gsub(&quot;ChEMBL&quot;, &quot;;&quot;,temp) temp &lt;- gsub(&quot;ChEBI&quot;, &quot;;&quot;,temp) Synonyms[((i-1)*100+1):(i*100)] &lt;- gsub(&quot;HMDB&quot;, &quot;;&quot;,temp) Chemical_Formula[((i-1)*100+1):(i*100)] &lt;- gsub(&quot;Chemical Formula&quot;, &quot;&quot;,node1[grep(&quot;Chemical Formula&quot;, node1)]) Monoisotopic_Molecular_Weight[((i-1)*100+1):(i*100)] &lt;- gsub(&quot;Monoisotopic Molecular Weight&quot;, &quot;&quot;,node1[grep(&quot;Monoisotopic Molecular Weight&quot;, node1)]) IUPAC_Name[((i-1)*100+1):(i*100)] &lt;- gsub(&quot;IUPAC Name&quot;, &quot;&quot;,node1[grep(&quot;IUPAC Name&quot;, node1)]) Traditional_Name[((i-1)*100+1):(i*100)] &lt;- gsub(&quot;Traditional Name&quot;, &quot;&quot;,node1[grep(&quot;Traditional Name&quot;, node1)]) CAS_Registry_Number[((i-1)*100+1):(i*100)] &lt;- gsub(&quot;CAS Registry Number&quot;, &quot;&quot;,node1[grep(&quot;CAS Registry Number&quot;, node1)]) Origin[((i-1)*100+1):(i*100)] &lt;- gsub(&quot;Origin&quot;, &quot;&quot;,node1[grep(&quot;Origin&quot;, node1)]) path &lt;- gsub(&quot;PathwaysNameSMPDB LinkKEGG Link&quot;, &quot;&quot;,node1[grep(&quot;Pathways&quot;, node1)]) Pathways[((i-1)*100+1):(i*100)] &lt;- substr(path, 1, stop = regexpr(&quot;SMP&quot;, path)-1) ##每100次保存一次 if (i*100 %in% seq(100, 60000, by = 100)) &#123; cat(&quot;save data...\n&quot;) save(HMDB_ID, Common_Name, Synonyms, Chemical_Formula, Monoisotopic_Molecular_Weight, IUPAC_Name, Traditional_Name, CAS_Registry_Number, Origin, Pathways, file = paste(&quot;hmdb.data&quot;,i*100)) send.mail(from = &quot;yourmail1@163.com&quot;, to = c(&quot;youmail20@163.com&quot;), subject = paste(&quot;WZZ GO ON:&quot;, i), body = paste(&quot;WZZ still go on&quot;, i), smtp = list(host.name = &quot;smtp.163.com&quot;, port = 465, user.name = &quot;yourmail1&quot;, passwd = &quot;passward&quot;, ssl = TRUE), authenticate = TRUE, send = TRUE) &#125;&#125; 因为代谢物信息比较大，可能需要一晚上，因此想到了没爬取100个，就给自己发一封邮件，来对程序进行监控。 写的比较粗糙，有时间再好好修改一下。]]></content>
  </entry>
  <entry>
    <title><![CDATA[2017年的第一篇博客]]></title>
    <url>%2F2017%2F09%2F09%2F2017-02-12-firstblog2017%2F</url>
    <content type="text"><![CDATA[我的2016年 现在想想，其实整个2016年真的是没什么收获，科研上没什么进展，技能上也没有什么提升，原本准备要学习的python，也在1-2个月之后，彻底的放弃了。现在可能只记得一点点皮毛了。那么2016年的问题到底在哪里呢？总结来说，可以分为下面几部分。 效率低下不得不承认，我的学习效率真的是出奇的低下，从我上高中以来其实都是这样了，做什么事情都不能够专心致志。用高中同学利弟的话来说，是“玩的时候没有好好玩，学习的时候没有好好学习”，用最近学到的一句话是“用战术上的勤奋来掩饰战略上的懒惰”。具体表现就是精神不集中，开小车。 懒惰现在真的是越来越懒惰了，很多事情都没有以前有激情了，可能是我老了？ 没有明确的目标上半年其实还是挺有目标的，但是到了下半年，目标突然就失焦了。定下一个目标，努力去实现，或者定下一个计划，努力去完成。 上面就是我2016年的一个简短的总结了，当然不是只有这些问题，还有很多其他的问题，但是可能都是一些细节的问题，就不过多去写了。 我的2017年计划 文章很幸运的是自己还是发了两篇文章，但是都是很低分的，虽然够我毕业，但是其实自己知道并不足够，因此2017年的主要任务就是好好看文献，做实验，寻找思路，争取能够再发一篇高质量的你文章。 统计毕竟我的课题和统计关系很大，自己的统计数据功底很差，希望可以在看文献以及资料的同时，好好学习一下统计，机器学习等知识，再课题中能够真正运用到。 RR是一个好东西，需要再深入的学习，主要是结合着统计去学习，并及时做好总计，以有道笔记和博客作为平台去记录自己的学习，即作为笔记也作为总结。 python这个放到最后，是因为可能很难有很多时间去学习了，希望前面几条比较顺利，才会有时间去做。 健身经过别人的提醒，还有这一项，确实自己看起来太瘦了，尤其是上半身，连衬衫和西装都撑不起来，等天气开始暖和了，就开始跑步，然后等中期答辩结束之后，考虑办一张健身卡，去健身！ 最后的最后，提醒自己，最后一年，加油！]]></content>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
</search>
